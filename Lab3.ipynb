{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import torch\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x initialized: \n",
      "-2.9273e+15  3.0688e-41  2.9122e+26\n",
      " 4.5747e-41  1.0222e+22  4.5747e-41\n",
      "-1.4727e+15  3.0688e-41 -1.4727e+15\n",
      " 3.0688e-41 -1.4727e+15  3.0688e-41\n",
      "-1.4728e+15  3.0688e-41  2.9791e+26\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "x type: <class 'torch.FloatTensor'>\n"
     ]
    }
   ],
   "source": [
    "#1\n",
    "x = torch.Tensor(5, 3)\n",
    "print (\"x initialized:\",x)\n",
    "print (\"x type:\",type(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random value of y: \n",
      " 0.9654  0.2305  0.0283\n",
      " 0.4412  0.3733  0.9091\n",
      " 0.3390  0.8753  0.2605\n",
      " 0.9858  0.1839  0.1120\n",
      " 0.4427  0.8682  0.5169\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "We can see that all value in y will be positive.\n",
      "<class 'torch.FloatTensor'>\n",
      "\n",
      "-0.4726 -0.3561 -0.5370\n",
      "-1.5538  0.4819  1.3962\n",
      " 0.5589 -0.4778  1.5532\n",
      " 0.0937 -0.6830 -1.6413\n",
      "-0.5698  0.5418 -1.5121\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "We can see that using randn will create negitive value.\n"
     ]
    }
   ],
   "source": [
    "#2\n",
    "y = torch.rand(5,3)\n",
    "print(\"Random value of y:\",y)\n",
    "print(\"We can see that all value in y will be positive.\")\n",
    "print(type(y))\n",
    "y2 = torch.randn(5,3)\n",
    "print(y2)\n",
    "print(\"We can see that using randn will create negitive value.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: \n",
      "-2.9273e+15  3.0688e-41  2.9122e+26\n",
      " 4.5747e-41  1.0222e+22  4.5747e-41\n",
      "-1.4727e+15  3.0688e-41 -1.4727e+15\n",
      " 3.0688e-41 -1.4727e+15  3.0688e-41\n",
      "-1.4728e+15  3.0688e-41  2.9791e+26\n",
      "[torch.DoubleTensor of size 5x3]\n",
      "\n",
      "y: \n",
      " 0.9654  0.2305  0.0283\n",
      " 0.4412  0.3733  0.9091\n",
      " 0.3390  0.8753  0.2605\n",
      " 0.9858  0.1839  0.1120\n",
      " 0.4427  0.8682  0.5169\n",
      "[torch.DoubleTensor of size 5x3]\n",
      "\n",
      "x type: <class 'torch.DoubleTensor'>\n",
      "y type: <class 'torch.DoubleTensor'>\n"
     ]
    }
   ],
   "source": [
    "#3\n",
    "x = x.double()\n",
    "y = y.double()\n",
    "print(\"x:\",x)\n",
    "print(\"y:\",y)\n",
    "print(\"x type:\",type(x))\n",
    "print(\"y type:\",type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape: torch.Size([5, 3])\n",
      "y shape: torch.Size([5, 3])\n"
     ]
    }
   ],
   "source": [
    "#4\n",
    "x = torch . Tensor ([[ -0.1859 , 1.3970 , 0.5236] ,\n",
    "[ 2.3854 , 0.0707 , 2.1970] ,\n",
    "[ -0.3587 , 1.2359 , 1.8951] ,\n",
    "[ -0.1189 , -0.1376 , 0.4647] ,\n",
    "[ -1.8968 , 2.0164 , 0.1092]])\n",
    "y = torch . Tensor ([[ 0.4838 , 0.5822 , 0.2755] ,\n",
    "[ 1.0982 , 0.4932 , -0.6680] ,\n",
    "[ 0.7915 , 0.6580 , -0.5819] ,\n",
    "[ 0.3825 , -1.1822 , 1.5217] ,\n",
    "[ 0.6042 , -0.2280 , 1.3210]])\n",
    "print(\"x shape:\",x.shape)\n",
    "print(\"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z: \n",
      "(0 ,.,.) = \n",
      " -0.1859  1.3970  0.5236\n",
      "  2.3854  0.0707  2.1970\n",
      " -0.3587  1.2359  1.8951\n",
      " -0.1189 -0.1376  0.4647\n",
      " -1.8968  2.0164  0.1092\n",
      "\n",
      "(1 ,.,.) = \n",
      "  0.4838  0.5822  0.2755\n",
      "  1.0982  0.4932 -0.6680\n",
      "  0.7915  0.6580 -0.5819\n",
      "  0.3825 -1.1822  1.5217\n",
      "  0.6042 -0.2280  1.3210\n",
      "[torch.FloatTensor of size 2x5x3]\n",
      "\n",
      "z shape: torch.Size([2, 5, 3])\n",
      "cat 0: \n",
      "-0.1859  1.3970  0.5236\n",
      " 2.3854  0.0707  2.1970\n",
      "-0.3587  1.2359  1.8951\n",
      "-0.1189 -0.1376  0.4647\n",
      "-1.8968  2.0164  0.1092\n",
      " 0.4838  0.5822  0.2755\n",
      " 1.0982  0.4932 -0.6680\n",
      " 0.7915  0.6580 -0.5819\n",
      " 0.3825 -1.1822  1.5217\n",
      " 0.6042 -0.2280  1.3210\n",
      "[torch.FloatTensor of size 10x3]\n",
      "\n",
      "cat 1: \n",
      "-0.1859  1.3970  0.5236  0.4838  0.5822  0.2755\n",
      " 2.3854  0.0707  2.1970  1.0982  0.4932 -0.6680\n",
      "-0.3587  1.2359  1.8951  0.7915  0.6580 -0.5819\n",
      "-0.1189 -0.1376  0.4647  0.3825 -1.1822  1.5217\n",
      "-1.8968  2.0164  0.1092  0.6042 -0.2280  1.3210\n",
      "[torch.FloatTensor of size 5x6]\n",
      "\n",
      "The shape of z is different from torch.cat((x, y), 0)) and torch.cat((x, y), 1))\n",
      "But the size are the same.\n"
     ]
    }
   ],
   "source": [
    "#5\n",
    "z = torch.stack((x,y))\n",
    "print(\"z:\",z)\n",
    "print(\"z shape:\",z.shape)\n",
    "print(\"cat 0:\", torch.cat((x, y), 0))\n",
    "print(\"cat 1:\", torch.cat((x, y), 1))\n",
    "print(\"The shape of z is different from torch.cat((x, y), 0)) and torch.cat((x, y), 1))\")\n",
    "print(\"But the size are the same.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5th row, 3rd col of y: 1.32099997997\n",
      "The same value in z is at the position of [1,4,2] and the value is: 1.32099997997\n"
     ]
    }
   ],
   "source": [
    "#6\n",
    "print(\"5th row, 3rd col of y:\",y[4,2])\n",
    "print(\"The same value in z is at the position of [1,4,2] and the value is:\",z[1,4,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.109200000763\n",
      "1.32099997997\n",
      "Number of element in 5th row and 3rd column in z: 2\n"
     ]
    }
   ],
   "source": [
    "#7\n",
    "count = 0\n",
    "for i in range(len(z[:])):\n",
    "    print(z[i,4,2])\n",
    "    count = count + 1\n",
    "print(\"Number of element in 5th row and 3rd column in z:\", count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.2979  1.9792  0.7991\n",
      " 3.4836  0.5639  1.5290\n",
      " 0.4328  1.8939  1.3132\n",
      " 0.2636 -1.3198  1.9864\n",
      "-1.2926  1.7884  1.4302\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "\n",
      " 0.2979  1.9792  0.7991\n",
      " 3.4836  0.5639  1.5290\n",
      " 0.4328  1.8939  1.3132\n",
      " 0.2636 -1.3198  1.9864\n",
      "-1.2926  1.7884  1.4302\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "\n",
      " 0.2979  1.9792  0.7991\n",
      " 3.4836  0.5639  1.5290\n",
      " 0.4328  1.8939  1.3132\n",
      " 0.2636 -1.3198  1.9864\n",
      "-1.2926  1.7884  1.4302\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "\n",
      " 0.2979  1.9792  0.7991\n",
      " 3.4836  0.5639  1.5290\n",
      " 0.4328  1.8939  1.3132\n",
      " 0.2636 -1.3198  1.9864\n",
      "-1.2926  1.7884  1.4302\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "Yes they all have same result, except that using torch.add with out, the result will update on the given parameter, in this case is x\n"
     ]
    }
   ],
   "source": [
    "#8\n",
    "print(x + y)\n",
    "print(torch.add(x, y))\n",
    "print(x.add(y))\n",
    "torch.add(x,y,out = x)\n",
    "print(x)\n",
    "print(\"Yes they all have same result, except that using torch.add with out, the result will update on the given parameter, in this case is x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])\n",
      "Using -1 means that it is infereed from other dimenstions\n"
     ]
    }
   ],
   "source": [
    "#9\n",
    "x = torch.randn(4, 4)\n",
    "y = x.view(16)\n",
    "z = x.view(-1, 8)\n",
    "print(x. size(), y.size(), z.size())\n",
    "print('Using -1 means that it is infereed from other dimenstions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-2.7125  7.1521\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#10\n",
    "x = torch.randn(10,10)\n",
    "y = torch.randn(2,100)\n",
    "x = x.view(1,100)\n",
    "y = y.view(100,2)\n",
    "resize = torch.mm(x,y)\n",
    "print(resize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: \n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "a type: <class 'torch.FloatTensor'>\n",
      "b: [1. 1. 1. 1. 1.]\n",
      "b type: <type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "#11\n",
    "a = torch . ones (5)\n",
    "print (\"a:\",a)\n",
    "print(\"a type:\", type(a))\n",
    "b = a . numpy ()\n",
    "print (\"b:\",b)\n",
    "print(\"b type:\", type(b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: \n",
      " 2\n",
      " 1\n",
      " 1\n",
      " 1\n",
      " 1\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "b: [2. 1. 1. 1. 1.]\n",
      "Memory address of a: 140213801749752\n",
      "Memory address of b: 140213798702976\n",
      "Yes they are match. When a has change, the according variable will be update in b.\n",
      "But the memory of a and b are different\n"
     ]
    }
   ],
   "source": [
    "#12\n",
    "a[0] += 1\n",
    "print(\"a:\",a)\n",
    "print(\"b:\",b)\n",
    "print(\"Memory address of a:\",id(a))\n",
    "print(\"Memory address of b:\",id(b))\n",
    "print(\"Yes they are match. When a has change, the according variable will be update in b.\")\n",
    "print(\"But the memory of a and b are different\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: \n",
      " 3\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "b: [3. 2. 2. 2. 2.]\n",
      "Memory address of a: 140213801749752\n",
      "Memory address of b: 140213798702976\n",
      "a: \n",
      " 3\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "b: [3. 2. 2. 2. 2.]\n",
      "Memory address of a: 140213801749752\n",
      "Memory address of b: 140213798702976\n",
      "a: \n",
      " 3\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      "[torch.FloatTensor of size 5]\n",
      "\n",
      "b: [2. 1. 1. 1. 1.]\n",
      "Memory address of a: 140213800802568\n",
      "Memory address of b: 140213798702976\n",
      "Using add_ is the same as +=, which we can see that a and b both update value and the memory address remain the same.\n",
      "However, using add function is different from the above, which b did not change when a + 1, also the memory address is different as well.\n"
     ]
    }
   ],
   "source": [
    "#13\n",
    "a.add_(1)\n",
    "print(\"a:\",a)\n",
    "print(\"b:\",b)\n",
    "print(\"Memory address of a:\",id(a))\n",
    "print(\"Memory address of b:\",id(b))\n",
    "a.add_(-1)\n",
    "a[:] += 1\n",
    "print(\"a:\",a)\n",
    "print(\"b:\",b)\n",
    "print(\"Memory address of a:\",id(a))\n",
    "print(\"Memory address of b:\",id(b))\n",
    "a[:] -= 1\n",
    "a = a.add(1)\n",
    "print(\"a:\",a)\n",
    "print(\"b:\",b)\n",
    "print(\"Memory address of a:\",id(a))\n",
    "print(\"Memory address of b:\",id(b))\n",
    "print(\"Using add_ is the same as +=, which we can see that a and b both update value and the memory address remain the same.\")\n",
    "print(\"However, using add function is different from the above, which b did not change when a + 1, also the memory address is different as well.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.]\n",
      "\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      " 2\n",
      "[torch.DoubleTensor of size 5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#14\n",
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1.0954  0.6907 -0.4844\n",
      "-0.2926  3.3236  0.4619\n",
      " 0.3603 -1.9459 -0.5833\n",
      "-1.1365 -0.9349 -0.2302\n",
      " 0.9912 -0.4061  1.0051\n",
      "[torch.cuda.FloatTensor of size 5x3 (GPU 0)]\n",
      "\n",
      "\n",
      " 1.0954  0.6907 -0.4844\n",
      "-0.2926  3.3236  0.4619\n",
      " 0.3603 -1.9459 -0.5833\n",
      "-1.1365 -0.9349 -0.2302\n",
      " 0.9912 -0.4061  1.0051\n",
      "[torch.FloatTensor of size 5x3]\n",
      "\n",
      "According to the result, yes it will still work on a CPU-only architecture\n"
     ]
    }
   ],
   "source": [
    "#15\n",
    "x = torch.randn(5 , 3)\n",
    "y = torch.randn(5 , 3)\n",
    "if torch.cuda.is_available():\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "z = x + y\n",
    "print (z)\n",
    "if torch.cuda.is_available():\n",
    "    print (z.cpu())\n",
    "print(\"According to the result, yes it will still work on a CPU-only architecture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Since z has already used cuda, we need to use z.cpu() to operate z.\n",
      "[[ 1.0954027   0.69066775 -0.484442  ]\n",
      " [-0.2925557   3.3236303   0.46194667]\n",
      " [ 0.3602511  -1.9459293  -0.58330953]\n",
      " [-1.136477   -0.9348796  -0.2301648 ]\n",
      " [ 0.99120057 -0.4061454   1.0050899 ]]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "can't convert CUDA tensor to numpy (it doesn't support GPU arrays). Use .cpu() to move the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-16ff7a274177>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Since z has already used cuda, we need to use z.cpu() to operate z.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: can't convert CUDA tensor to numpy (it doesn't support GPU arrays). Use .cpu() to move the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "#16\n",
    "print(\"Since z has already used cuda, we need to use z.cpu() to operate z.\")\n",
    "print(z.cpu().numpy())\n",
    "print(z.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 1  1\n",
      " 1  1\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "Variable containing:\n",
      " 3  3\n",
      " 3  3\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "y type: <class 'torch.autograd.variable.Variable'>\n"
     ]
    }
   ],
   "source": [
    "#17\n",
    "import torch.autograd as ag\n",
    "x = ag.Variable(torch.ones(2,2), requires_grad=True)\n",
    "print(x)\n",
    "y = x + 2\n",
    "print(y)\n",
    "print(\"y type:\", type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z: Variable containing:\n",
      " 27  27\n",
      " 27  27\n",
      "[torch.FloatTensor of size 2x2]\n",
      " f: Variable containing:\n",
      " 27\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#18\n",
    "z = y*y*3\n",
    "f = z.mean()\n",
    "print(\"z:\", z, \"f:\", f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 4.5000  4.5000\n",
      " 4.5000  4.5000\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#19\n",
    "f.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MNISTtools' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0a37a8ebc88b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mx_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mM\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m# normalize and do the minus 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx_res\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mxtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mltrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMNISTtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"training\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/datasets/MNIST\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mxtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mltest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMNISTtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"testing\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/datasets/MNIST\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MNISTtools' is not defined"
     ]
    }
   ],
   "source": [
    "#21\n",
    "import MNISTtools\n",
    "def normalize_MNIST_images(x):\n",
    "    m = x.min()\n",
    "    M = x.max()\n",
    "    x = x.astype(np.float64) # turn data from int to float\n",
    "    x_res = 2*((x - m).astype(np.float64)/(M - m)) - 1 # normalize and do the minus 1\n",
    "    return x_res\n",
    "xtrain, ltrain = MNISTtools.load(dataset = \"training\", path = \"/datasets/MNIST\")\n",
    "xtest, ltest = MNISTtools.load(dataset = \"testing\", path = \"/datasets/MNIST\")\n",
    "\n",
    "#normalize\n",
    "xtrain = normalize_MNIST_images(xtrain)\n",
    "xtest = normalize_MNIST_images(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtrain before reshape: (784, 60000)\n",
      "xtrain after reshape: (60000, 1, 28, 28)\n",
      "xtest before reshape: (784, 10000)\n",
      "xtest before reshape: (10000, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "#22\n",
    "print(\"xtrain before reshape:\", xtrain.shape)\n",
    "xtrain = xtrain.reshape(28,28,1,60000)\n",
    "xtrain = np.moveaxis(xtrain,-1,0)\n",
    "xtrain = np.moveaxis(xtrain,-1,1)\n",
    "print(\"xtrain after reshape:\", xtrain.shape)\n",
    "print(\"xtest before reshape:\", xtest.shape)\n",
    "xtest = xtest.reshape(28,28,1,10000)\n",
    "xtest = np.moveaxis(xtest,-1,0)\n",
    "xtest = np.moveaxis(xtest,-1,1)\n",
    "print(\"xtest before reshape:\", xtest.shape)\n",
    "plot_x = xtrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#23\n",
    "import matplotlib as pyplot\n",
    "MNISTtools.show(plot_x[42 , 0 , : , :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#24\n",
    "xtrain = ag.Variable(torch.from_numpy(xtrain),requires_grad = True)\n",
    "ltrain = ag.Variable(torch.from_numpy(ltrain),requires_grad = False)\n",
    "xtest = ag.Variable(torch.from_numpy(xtest), requires_grad = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(a) size of feature map after convolution: 6 X 24 X 24\n",
      "(a) size of feature map after maxpooling: 6 X 12 X 12\n",
      "(b) size of feature map after convolution: 16 X 8 X 8\n",
      "(b) size of feature map after maxpooling: 16 X 4 X 4\n",
      "There are  256  units in the third layer\n"
     ]
    }
   ],
   "source": [
    "#25\n",
    "print(\"(a) size of feature map after convolution: 6 X\",(28-5+1),\"X\",(28-5+1))\n",
    "print(\"(a) size of feature map after maxpooling: 6 X\",(24/2),\"X\",(24/2))\n",
    "\n",
    "print(\"(b) size of feature map after convolution: 16 X\",(12-5+1),\"X\",(12-5+1))\n",
    "print(\"(b) size of feature map after maxpooling: 16 X\",(8/2),\"X\",(8/2))\n",
    "\n",
    "print(\"There are \",16*4*4,\" units in the third layer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#26\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1,6,5).double()\n",
    "        self.conv2 = nn.Conv2d(6,16,5).double()\n",
    "        self.fc1 = nn.Linear(16*16,120).double()\n",
    "        self.fc2 = nn.Linear(120,84).double()\n",
    "        self.fc3 = nn.Linear(84, 10).double()\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2,2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        return np.prod(size)\n",
    "net = LeNet()\n",
    "print(net)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 torch.Size([6, 1, 5, 5])\n",
      "1 torch.Size([6])\n",
      "2 torch.Size([16, 6, 5, 5])\n",
      "3 torch.Size([16])\n",
      "4 torch.Size([120, 256])\n",
      "5 torch.Size([120])\n",
      "6 torch.Size([84, 120])\n",
      "7 torch.Size([84])\n",
      "8 torch.Size([10, 84])\n",
      "9 torch.Size([10])\n",
      "We can see that 0 and 2 have 4 parameters, 4,6,8 have 2 parameters and all the odd indices have only one parameters\n"
     ]
    }
   ],
   "source": [
    "#27\n",
    "params = list(net.parameters())\n",
    "for i in range(len(params)):\n",
    "    print(i, params[i].size())\n",
    "print(\"We can see that 0 and 2 have 4 parameters, 4,6,8 have 2 parameters and all the odd indices have only one parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.43\n"
     ]
    }
   ],
   "source": [
    "#28\n",
    "yinit = net.forward(xtest)\n",
    "print(100 * np.mean(ltest == yinit.data.numpy().T.argmax(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#29\n",
    "N = xtrain.size()[0]\n",
    "B = 100\n",
    "NB = N/B\n",
    "T = 10\n",
    "gamma = .001\n",
    "rho = .9\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr = gamma, momentum = rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 2.303\n",
      "[1,   200] loss: 2.298\n",
      "[1,   300] loss: 2.291\n",
      "[1,   400] loss: 2.283\n",
      "[1,   500] loss: 2.265\n",
      "[1,   600] loss: 2.229\n",
      "[2,   100] loss: 2.130\n",
      "[2,   200] loss: 1.746\n",
      "[2,   300] loss: 0.982\n",
      "[2,   400] loss: 0.591\n",
      "[2,   500] loss: 0.439\n",
      "[2,   600] loss: 0.374\n",
      "[3,   100] loss: 0.314\n",
      "[3,   200] loss: 0.288\n",
      "[3,   300] loss: 0.278\n",
      "[3,   400] loss: 0.268\n",
      "[3,   500] loss: 0.225\n",
      "[3,   600] loss: 0.232\n",
      "[4,   100] loss: 0.207\n",
      "[4,   200] loss: 0.190\n",
      "[4,   300] loss: 0.192\n",
      "[4,   400] loss: 0.185\n",
      "[4,   500] loss: 0.164\n",
      "[4,   600] loss: 0.173\n",
      "[5,   100] loss: 0.159\n",
      "[5,   200] loss: 0.148\n",
      "[5,   300] loss: 0.157\n",
      "[5,   400] loss: 0.147\n",
      "[5,   500] loss: 0.139\n",
      "[5,   600] loss: 0.123\n",
      "[6,   100] loss: 0.126\n",
      "[6,   200] loss: 0.129\n",
      "[6,   300] loss: 0.112\n",
      "[6,   400] loss: 0.116\n",
      "[6,   500] loss: 0.121\n",
      "[6,   600] loss: 0.125\n",
      "[7,   100] loss: 0.112\n",
      "[7,   200] loss: 0.115\n",
      "[7,   300] loss: 0.110\n",
      "[7,   400] loss: 0.111\n",
      "[7,   500] loss: 0.099\n",
      "[7,   600] loss: 0.092\n",
      "[8,   100] loss: 0.099\n",
      "[8,   200] loss: 0.093\n",
      "[8,   300] loss: 0.091\n",
      "[8,   400] loss: 0.102\n",
      "[8,   500] loss: 0.096\n",
      "[8,   600] loss: 0.097\n",
      "[9,   100] loss: 0.086\n",
      "[9,   200] loss: 0.089\n",
      "[9,   300] loss: 0.094\n",
      "[9,   400] loss: 0.091\n",
      "[9,   500] loss: 0.089\n",
      "[9,   600] loss: 0.084\n",
      "[10,   100] loss: 0.079\n",
      "[10,   200] loss: 0.082\n",
      "[10,   300] loss: 0.082\n",
      "[10,   400] loss: 0.083\n",
      "[10,   500] loss: 0.077\n",
      "[10,   600] loss: 0.090\n",
      "finished training\n"
     ]
    }
   ],
   "source": [
    "#30\n",
    "for epoch in range(T):\n",
    "    running_loss = 0.0\n",
    "    idxminibatches = np.random.permutation(NB)\n",
    "    for k in range(NB):\n",
    "        i = idxminibatches[k]\n",
    "        idxsmp = range(B*i, B*(i+1))\n",
    "        inputs = xtrain[idxsmp]\n",
    "        labels = ltrain[idxsmp]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        running_loss += loss[0]\n",
    "        if k % 100 == 99:\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, k + 1, running_loss/100))\n",
    "            running_loss = 0.0\n",
    "            \n",
    "print('finished training')\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 2.304\n",
      "[1,   200] loss: 2.296\n",
      "[1,   300] loss: 2.287\n",
      "[1,   400] loss: 2.273\n",
      "[1,   500] loss: 2.240\n",
      "[1,   600] loss: 2.150\n",
      "[2,   100] loss: 1.787\n",
      "[2,   200] loss: 1.065\n",
      "[2,   300] loss: 0.674\n",
      "[2,   400] loss: 0.517\n",
      "[2,   500] loss: 0.439\n",
      "[2,   600] loss: 0.375\n",
      "[3,   100] loss: 0.344\n",
      "[3,   200] loss: 0.312\n",
      "[3,   300] loss: 0.274\n",
      "[3,   400] loss: 0.258\n",
      "[3,   500] loss: 0.247\n",
      "[3,   600] loss: 0.220\n",
      "[4,   100] loss: 0.216\n",
      "[4,   200] loss: 0.191\n",
      "[4,   300] loss: 0.194\n",
      "[4,   400] loss: 0.186\n",
      "[4,   500] loss: 0.154\n",
      "[4,   600] loss: 0.171\n",
      "[5,   100] loss: 0.161\n",
      "[5,   200] loss: 0.153\n",
      "[5,   300] loss: 0.138\n",
      "[5,   400] loss: 0.146\n",
      "[5,   500] loss: 0.136\n",
      "[5,   600] loss: 0.135\n",
      "[6,   100] loss: 0.126\n",
      "[6,   200] loss: 0.132\n",
      "[6,   300] loss: 0.129\n",
      "[6,   400] loss: 0.121\n",
      "[6,   500] loss: 0.106\n",
      "[6,   600] loss: 0.114\n",
      "[7,   100] loss: 0.108\n",
      "[7,   200] loss: 0.108\n",
      "[7,   300] loss: 0.111\n",
      "[7,   400] loss: 0.108\n",
      "[7,   500] loss: 0.105\n",
      "[7,   600] loss: 0.096\n",
      "[8,   100] loss: 0.100\n",
      "[8,   200] loss: 0.092\n",
      "[8,   300] loss: 0.091\n",
      "[8,   400] loss: 0.088\n",
      "[8,   500] loss: 0.091\n",
      "[8,   600] loss: 0.103\n",
      "[9,   100] loss: 0.093\n",
      "[9,   200] loss: 0.091\n",
      "[9,   300] loss: 0.089\n",
      "[9,   400] loss: 0.086\n",
      "[9,   500] loss: 0.086\n",
      "[9,   600] loss: 0.075\n",
      "[10,   100] loss: 0.071\n",
      "[10,   200] loss: 0.079\n",
      "[10,   300] loss: 0.080\n",
      "[10,   400] loss: 0.087\n",
      "[10,   500] loss: 0.082\n",
      "[10,   600] loss: 0.085\n",
      "finished training\n"
     ]
    }
   ],
   "source": [
    "#31\n",
    "#load data into GPU\n",
    "xtrain, ltrain = MNISTtools.load(dataset = \"training\", path = \"/datasets/MNIST\")\n",
    "xtest, ltest = MNISTtools.load(dataset = \"testing\", path = \"/datasets/MNIST\")\n",
    "xtrain = normalize_MNIST_images(xtrain)\n",
    "xtest = normalize_MNIST_images(xtest)\n",
    "\n",
    "xtrain = xtrain.reshape(28,28,1,60000)\n",
    "xtrain = np.moveaxis(xtrain,-1,0)\n",
    "xtrain = np.moveaxis(xtrain,-1,1)\n",
    "xtest = xtest.reshape(28,28,1,10000)\n",
    "xtest = np.moveaxis(xtest,-1,0)\n",
    "xtest = np.moveaxis(xtest,-1,1)\n",
    "#xtrain and xtest add cuda to run in GPU\n",
    "xtrain = ag.Variable(torch.from_numpy(xtrain).cuda(),requires_grad = True)\n",
    "ltrain = ag.Variable(torch.from_numpy(ltrain),requires_grad = False)\n",
    "xtest = ag.Variable(torch.from_numpy(xtest).cuda(), requires_grad = True)\n",
    "#net add cuda to run in GPU\n",
    "net = LeNet()\n",
    "net.cuda()\n",
    "\n",
    "N = xtrain.size()[0]\n",
    "B = 100\n",
    "NB = N/B\n",
    "T = 10\n",
    "gamma = .001\n",
    "rho = .9\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr = gamma, momentum = rho)\n",
    "\n",
    "for epoch in range(T):\n",
    "    running_loss = 0.0\n",
    "    idxminibatches = np.random.permutation(NB)\n",
    "    for k in range(NB):\n",
    "        i = idxminibatches[k]\n",
    "        idxsmp = range(B*i, B*(i+1))\n",
    "        inputs = xtrain[idxsmp].cuda()\n",
    "        labels = ltrain[idxsmp].cuda()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        running_loss += loss[0]\n",
    "        if k % 100 == 99:\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, k + 1, running_loss/100))\n",
    "            running_loss = 0.0\n",
    "            \n",
    "print('finished training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97.89999999999999\n"
     ]
    }
   ],
   "source": [
    "#32\n",
    "yinit = net.forward(xtest)\n",
    "print(100 * np.mean(ltest == yinit.data.cpu().numpy().T.argmax(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 2.306\n",
      "[1,   200] loss: 2.296\n",
      "[1,   300] loss: 2.287\n",
      "[1,   400] loss: 2.272\n",
      "[1,   500] loss: 2.242\n",
      "[1,   600] loss: 2.161\n",
      "[1,   700] loss: 1.837\n",
      "[1,   800] loss: 1.007\n",
      "[1,   900] loss: 0.635\n",
      "[1,  1000] loss: 0.512\n",
      "[1,  1100] loss: 0.440\n",
      "[1,  1200] loss: 0.368\n",
      "[2,   100] loss: 0.326\n",
      "[2,   200] loss: 0.319\n",
      "[2,   300] loss: 0.307\n",
      "[2,   400] loss: 0.274\n",
      "[2,   500] loss: 0.255\n",
      "[2,   600] loss: 0.220\n",
      "[2,   700] loss: 0.228\n",
      "[2,   800] loss: 0.235\n",
      "[2,   900] loss: 0.203\n",
      "[2,  1000] loss: 0.193\n",
      "[2,  1100] loss: 0.179\n",
      "[2,  1200] loss: 0.176\n",
      "[3,   100] loss: 0.167\n",
      "[3,   200] loss: 0.160\n",
      "[3,   300] loss: 0.148\n",
      "[3,   400] loss: 0.150\n",
      "[3,   500] loss: 0.153\n",
      "[3,   600] loss: 0.151\n",
      "[3,   700] loss: 0.139\n",
      "[3,   800] loss: 0.135\n",
      "[3,   900] loss: 0.143\n",
      "[3,  1000] loss: 0.112\n",
      "[3,  1100] loss: 0.117\n",
      "[3,  1200] loss: 0.129\n",
      "[4,   100] loss: 0.119\n",
      "[4,   200] loss: 0.116\n",
      "[4,   300] loss: 0.119\n",
      "[4,   400] loss: 0.113\n",
      "[4,   500] loss: 0.109\n",
      "[4,   600] loss: 0.109\n",
      "[4,   700] loss: 0.112\n",
      "[4,   800] loss: 0.094\n",
      "[4,   900] loss: 0.094\n",
      "[4,  1000] loss: 0.097\n",
      "[4,  1100] loss: 0.099\n",
      "[4,  1200] loss: 0.109\n",
      "[5,   100] loss: 0.091\n",
      "[5,   200] loss: 0.087\n",
      "[5,   300] loss: 0.077\n",
      "[5,   400] loss: 0.101\n",
      "[5,   500] loss: 0.098\n",
      "[5,   600] loss: 0.074\n",
      "[5,   700] loss: 0.080\n",
      "[5,   800] loss: 0.109\n",
      "[5,   900] loss: 0.090\n",
      "[5,  1000] loss: 0.090\n",
      "[5,  1100] loss: 0.088\n",
      "[5,  1200] loss: 0.080\n",
      "[6,   100] loss: 0.083\n",
      "[6,   200] loss: 0.086\n",
      "[6,   300] loss: 0.079\n",
      "[6,   400] loss: 0.073\n",
      "[6,   500] loss: 0.077\n",
      "[6,   600] loss: 0.090\n",
      "[6,   700] loss: 0.069\n",
      "[6,   800] loss: 0.080\n",
      "[6,   900] loss: 0.073\n",
      "[6,  1000] loss: 0.072\n",
      "[6,  1100] loss: 0.079\n",
      "[6,  1200] loss: 0.075\n",
      "[7,   100] loss: 0.060\n",
      "[7,   200] loss: 0.097\n",
      "[7,   300] loss: 0.065\n",
      "[7,   400] loss: 0.067\n",
      "[7,   500] loss: 0.071\n",
      "[7,   600] loss: 0.065\n",
      "[7,   700] loss: 0.076\n",
      "[7,   800] loss: 0.063\n",
      "[7,   900] loss: 0.072\n",
      "[7,  1000] loss: 0.058\n",
      "[7,  1100] loss: 0.078\n",
      "[7,  1200] loss: 0.066\n",
      "[8,   100] loss: 0.060\n",
      "[8,   200] loss: 0.066\n",
      "[8,   300] loss: 0.052\n",
      "[8,   400] loss: 0.056\n",
      "[8,   500] loss: 0.061\n",
      "[8,   600] loss: 0.063\n",
      "[8,   700] loss: 0.065\n",
      "[8,   800] loss: 0.062\n",
      "[8,   900] loss: 0.060\n",
      "[8,  1000] loss: 0.074\n",
      "[8,  1100] loss: 0.071\n",
      "[8,  1200] loss: 0.070\n",
      "[9,   100] loss: 0.060\n",
      "[9,   200] loss: 0.058\n",
      "[9,   300] loss: 0.051\n",
      "[9,   400] loss: 0.056\n",
      "[9,   500] loss: 0.062\n",
      "[9,   600] loss: 0.056\n",
      "[9,   700] loss: 0.054\n",
      "[9,   800] loss: 0.056\n",
      "[9,   900] loss: 0.062\n",
      "[9,  1000] loss: 0.046\n",
      "[9,  1100] loss: 0.072\n",
      "[9,  1200] loss: 0.052\n",
      "[10,   100] loss: 0.052\n",
      "[10,   200] loss: 0.057\n",
      "[10,   300] loss: 0.053\n",
      "[10,   400] loss: 0.055\n",
      "[10,   500] loss: 0.058\n",
      "[10,   600] loss: 0.056\n",
      "[10,   700] loss: 0.054\n",
      "[10,   800] loss: 0.048\n",
      "[10,   900] loss: 0.049\n",
      "[10,  1000] loss: 0.050\n",
      "[10,  1100] loss: 0.053\n",
      "[10,  1200] loss: 0.049\n",
      "[11,   100] loss: 0.054\n",
      "[11,   200] loss: 0.041\n",
      "[11,   300] loss: 0.054\n",
      "[11,   400] loss: 0.056\n",
      "[11,   500] loss: 0.054\n",
      "[11,   600] loss: 0.046\n",
      "[11,   700] loss: 0.047\n",
      "[11,   800] loss: 0.049\n",
      "[11,   900] loss: 0.040\n",
      "[11,  1000] loss: 0.045\n",
      "[11,  1100] loss: 0.045\n",
      "[11,  1200] loss: 0.053\n",
      "[12,   100] loss: 0.039\n",
      "[12,   200] loss: 0.042\n",
      "[12,   300] loss: 0.040\n",
      "[12,   400] loss: 0.048\n",
      "[12,   500] loss: 0.052\n",
      "[12,   600] loss: 0.046\n",
      "[12,   700] loss: 0.043\n",
      "[12,   800] loss: 0.038\n",
      "[12,   900] loss: 0.052\n",
      "[12,  1000] loss: 0.050\n",
      "[12,  1100] loss: 0.039\n",
      "[12,  1200] loss: 0.048\n",
      "[13,   100] loss: 0.036\n",
      "[13,   200] loss: 0.038\n",
      "[13,   300] loss: 0.042\n",
      "[13,   400] loss: 0.043\n",
      "[13,   500] loss: 0.044\n",
      "[13,   600] loss: 0.045\n",
      "[13,   700] loss: 0.045\n",
      "[13,   800] loss: 0.049\n",
      "[13,   900] loss: 0.039\n",
      "[13,  1000] loss: 0.042\n",
      "[13,  1100] loss: 0.037\n",
      "[13,  1200] loss: 0.050\n",
      "[14,   100] loss: 0.039\n",
      "[14,   200] loss: 0.034\n",
      "[14,   300] loss: 0.040\n",
      "[14,   400] loss: 0.043\n",
      "[14,   500] loss: 0.036\n",
      "[14,   600] loss: 0.040\n",
      "[14,   700] loss: 0.041\n",
      "[14,   800] loss: 0.046\n",
      "[14,   900] loss: 0.041\n",
      "[14,  1000] loss: 0.036\n",
      "[14,  1100] loss: 0.043\n",
      "[14,  1200] loss: 0.036\n",
      "[15,   100] loss: 0.038\n",
      "[15,   200] loss: 0.040\n",
      "[15,   300] loss: 0.032\n",
      "[15,   400] loss: 0.038\n",
      "[15,   500] loss: 0.034\n",
      "[15,   600] loss: 0.040\n",
      "[15,   700] loss: 0.036\n",
      "[15,   800] loss: 0.040\n",
      "[15,   900] loss: 0.033\n",
      "[15,  1000] loss: 0.042\n",
      "[15,  1100] loss: 0.040\n",
      "[15,  1200] loss: 0.033\n",
      "[16,   100] loss: 0.034\n",
      "[16,   200] loss: 0.028\n",
      "[16,   300] loss: 0.042\n",
      "[16,   400] loss: 0.036\n",
      "[16,   500] loss: 0.031\n",
      "[16,   600] loss: 0.037\n",
      "[16,   700] loss: 0.030\n",
      "[16,   800] loss: 0.035\n",
      "[16,   900] loss: 0.031\n",
      "[16,  1000] loss: 0.041\n",
      "[16,  1100] loss: 0.040\n",
      "[16,  1200] loss: 0.027\n",
      "[17,   100] loss: 0.029\n",
      "[17,   200] loss: 0.032\n",
      "[17,   300] loss: 0.029\n",
      "[17,   400] loss: 0.029\n",
      "[17,   500] loss: 0.030\n",
      "[17,   600] loss: 0.035\n",
      "[17,   700] loss: 0.031\n",
      "[17,   800] loss: 0.034\n",
      "[17,   900] loss: 0.033\n",
      "[17,  1000] loss: 0.039\n",
      "[17,  1100] loss: 0.039\n",
      "[17,  1200] loss: 0.036\n",
      "[18,   100] loss: 0.027\n",
      "[18,   200] loss: 0.035\n",
      "[18,   300] loss: 0.026\n",
      "[18,   400] loss: 0.031\n",
      "[18,   500] loss: 0.029\n",
      "[18,   600] loss: 0.032\n",
      "[18,   700] loss: 0.033\n",
      "[18,   800] loss: 0.029\n",
      "[18,   900] loss: 0.030\n",
      "[18,  1000] loss: 0.035\n",
      "[18,  1100] loss: 0.032\n",
      "[18,  1200] loss: 0.036\n",
      "[19,   100] loss: 0.033\n",
      "[19,   200] loss: 0.025\n",
      "[19,   300] loss: 0.032\n",
      "[19,   400] loss: 0.030\n",
      "[19,   500] loss: 0.029\n",
      "[19,   600] loss: 0.022\n",
      "[19,   700] loss: 0.036\n",
      "[19,   800] loss: 0.032\n",
      "[19,   900] loss: 0.031\n",
      "[19,  1000] loss: 0.029\n",
      "[19,  1100] loss: 0.026\n",
      "[19,  1200] loss: 0.029\n",
      "[20,   100] loss: 0.030\n",
      "[20,   200] loss: 0.025\n",
      "[20,   300] loss: 0.028\n",
      "[20,   400] loss: 0.032\n",
      "[20,   500] loss: 0.030\n",
      "[20,   600] loss: 0.025\n",
      "[20,   700] loss: 0.021\n",
      "[20,   800] loss: 0.030\n",
      "[20,   900] loss: 0.033\n",
      "[20,  1000] loss: 0.029\n",
      "[20,  1100] loss: 0.031\n",
      "[20,  1200] loss: 0.025\n",
      "[21,   100] loss: 0.026\n",
      "[21,   200] loss: 0.023\n",
      "[21,   300] loss: 0.027\n",
      "[21,   400] loss: 0.021\n",
      "[21,   500] loss: 0.025\n",
      "[21,   600] loss: 0.026\n",
      "[21,   700] loss: 0.027\n",
      "[21,   800] loss: 0.028\n",
      "[21,   900] loss: 0.028\n",
      "[21,  1000] loss: 0.033\n",
      "[21,  1100] loss: 0.031\n",
      "[21,  1200] loss: 0.024\n",
      "[22,   100] loss: 0.025\n",
      "[22,   200] loss: 0.027\n",
      "[22,   300] loss: 0.025\n",
      "[22,   400] loss: 0.035\n",
      "[22,   500] loss: 0.020\n",
      "[22,   600] loss: 0.022\n",
      "[22,   700] loss: 0.028\n",
      "[22,   800] loss: 0.025\n",
      "[22,   900] loss: 0.024\n",
      "[22,  1000] loss: 0.025\n",
      "[22,  1100] loss: 0.026\n",
      "[22,  1200] loss: 0.021\n",
      "[23,   100] loss: 0.027\n",
      "[23,   200] loss: 0.026\n",
      "[23,   300] loss: 0.025\n",
      "[23,   400] loss: 0.025\n",
      "[23,   500] loss: 0.025\n",
      "[23,   600] loss: 0.019\n",
      "[23,   700] loss: 0.021\n",
      "[23,   800] loss: 0.029\n",
      "[23,   900] loss: 0.023\n",
      "[23,  1000] loss: 0.025\n",
      "[23,  1100] loss: 0.020\n",
      "[23,  1200] loss: 0.029\n",
      "[24,   100] loss: 0.025\n",
      "[24,   200] loss: 0.018\n",
      "[24,   300] loss: 0.023\n",
      "[24,   400] loss: 0.022\n",
      "[24,   500] loss: 0.022\n",
      "[24,   600] loss: 0.019\n",
      "[24,   700] loss: 0.018\n",
      "[24,   800] loss: 0.027\n",
      "[24,   900] loss: 0.026\n",
      "[24,  1000] loss: 0.023\n",
      "[24,  1100] loss: 0.022\n",
      "[24,  1200] loss: 0.028\n",
      "[25,   100] loss: 0.018\n",
      "[25,   200] loss: 0.020\n",
      "[25,   300] loss: 0.019\n",
      "[25,   400] loss: 0.022\n",
      "[25,   500] loss: 0.026\n",
      "[25,   600] loss: 0.018\n",
      "[25,   700] loss: 0.019\n",
      "[25,   800] loss: 0.022\n",
      "[25,   900] loss: 0.021\n",
      "[25,  1000] loss: 0.023\n",
      "[25,  1100] loss: 0.020\n",
      "[25,  1200] loss: 0.020\n",
      "[26,   100] loss: 0.016\n",
      "[26,   200] loss: 0.017\n",
      "[26,   300] loss: 0.023\n",
      "[26,   400] loss: 0.019\n",
      "[26,   500] loss: 0.016\n",
      "[26,   600] loss: 0.020\n",
      "[26,   700] loss: 0.018\n",
      "[26,   800] loss: 0.022\n",
      "[26,   900] loss: 0.018\n",
      "[26,  1000] loss: 0.027\n",
      "[26,  1100] loss: 0.022\n",
      "[26,  1200] loss: 0.026\n",
      "[27,   100] loss: 0.016\n",
      "[27,   200] loss: 0.018\n",
      "[27,   300] loss: 0.016\n",
      "[27,   400] loss: 0.023\n",
      "[27,   500] loss: 0.014\n",
      "[27,   600] loss: 0.016\n",
      "[27,   700] loss: 0.020\n",
      "[27,   800] loss: 0.023\n",
      "[27,   900] loss: 0.021\n",
      "[27,  1000] loss: 0.021\n",
      "[27,  1100] loss: 0.024\n",
      "[27,  1200] loss: 0.022\n",
      "[28,   100] loss: 0.020\n",
      "[28,   200] loss: 0.017\n",
      "[28,   300] loss: 0.014\n",
      "[28,   400] loss: 0.014\n",
      "[28,   500] loss: 0.020\n",
      "[28,   600] loss: 0.019\n",
      "[28,   700] loss: 0.024\n",
      "[28,   800] loss: 0.015\n",
      "[28,   900] loss: 0.020\n",
      "[28,  1000] loss: 0.022\n",
      "[28,  1100] loss: 0.022\n",
      "[28,  1200] loss: 0.017\n",
      "[29,   100] loss: 0.017\n",
      "[29,   200] loss: 0.012\n",
      "[29,   300] loss: 0.017\n",
      "[29,   400] loss: 0.017\n",
      "[29,   500] loss: 0.021\n",
      "[29,   600] loss: 0.016\n",
      "[29,   700] loss: 0.017\n",
      "[29,   800] loss: 0.017\n",
      "[29,   900] loss: 0.015\n",
      "[29,  1000] loss: 0.022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29,  1100] loss: 0.014\n",
      "[29,  1200] loss: 0.021\n",
      "[30,   100] loss: 0.020\n",
      "[30,   200] loss: 0.021\n",
      "[30,   300] loss: 0.019\n",
      "[30,   400] loss: 0.016\n",
      "[30,   500] loss: 0.014\n",
      "[30,   600] loss: 0.013\n",
      "[30,   700] loss: 0.016\n",
      "[30,   800] loss: 0.016\n",
      "[30,   900] loss: 0.015\n",
      "[30,  1000] loss: 0.016\n",
      "[30,  1100] loss: 0.016\n",
      "[30,  1200] loss: 0.019\n",
      "[31,   100] loss: 0.018\n",
      "[31,   200] loss: 0.018\n",
      "[31,   300] loss: 0.012\n",
      "[31,   400] loss: 0.013\n",
      "[31,   500] loss: 0.017\n",
      "[31,   600] loss: 0.015\n",
      "[31,   700] loss: 0.016\n",
      "[31,   800] loss: 0.014\n",
      "[31,   900] loss: 0.023\n",
      "[31,  1000] loss: 0.018\n",
      "[31,  1100] loss: 0.013\n",
      "[31,  1200] loss: 0.015\n",
      "[32,   100] loss: 0.015\n",
      "[32,   200] loss: 0.017\n",
      "[32,   300] loss: 0.015\n",
      "[32,   400] loss: 0.012\n",
      "[32,   500] loss: 0.021\n",
      "[32,   600] loss: 0.009\n",
      "[32,   700] loss: 0.016\n",
      "[32,   800] loss: 0.014\n",
      "[32,   900] loss: 0.019\n",
      "[32,  1000] loss: 0.018\n",
      "[32,  1100] loss: 0.016\n",
      "[32,  1200] loss: 0.017\n",
      "[33,   100] loss: 0.013\n",
      "[33,   200] loss: 0.014\n",
      "[33,   300] loss: 0.012\n",
      "[33,   400] loss: 0.017\n",
      "[33,   500] loss: 0.012\n",
      "[33,   600] loss: 0.013\n",
      "[33,   700] loss: 0.013\n",
      "[33,   800] loss: 0.013\n",
      "[33,   900] loss: 0.017\n",
      "[33,  1000] loss: 0.019\n",
      "[33,  1100] loss: 0.014\n",
      "[33,  1200] loss: 0.015\n",
      "[34,   100] loss: 0.010\n",
      "[34,   200] loss: 0.014\n",
      "[34,   300] loss: 0.013\n",
      "[34,   400] loss: 0.012\n",
      "[34,   500] loss: 0.012\n",
      "[34,   600] loss: 0.017\n",
      "[34,   700] loss: 0.013\n",
      "[34,   800] loss: 0.015\n",
      "[34,   900] loss: 0.012\n",
      "[34,  1000] loss: 0.015\n",
      "[34,  1100] loss: 0.013\n",
      "[34,  1200] loss: 0.017\n",
      "[35,   100] loss: 0.017\n",
      "[35,   200] loss: 0.014\n",
      "[35,   300] loss: 0.012\n",
      "[35,   400] loss: 0.015\n",
      "[35,   500] loss: 0.012\n",
      "[35,   600] loss: 0.014\n",
      "[35,   700] loss: 0.014\n",
      "[35,   800] loss: 0.014\n",
      "[35,   900] loss: 0.017\n",
      "[35,  1000] loss: 0.014\n",
      "[35,  1100] loss: 0.011\n",
      "[35,  1200] loss: 0.012\n",
      "[36,   100] loss: 0.009\n",
      "[36,   200] loss: 0.012\n",
      "[36,   300] loss: 0.013\n",
      "[36,   400] loss: 0.008\n",
      "[36,   500] loss: 0.013\n",
      "[36,   600] loss: 0.014\n",
      "[36,   700] loss: 0.011\n",
      "[36,   800] loss: 0.010\n",
      "[36,   900] loss: 0.013\n",
      "[36,  1000] loss: 0.015\n",
      "[36,  1100] loss: 0.013\n",
      "[36,  1200] loss: 0.012\n",
      "[37,   100] loss: 0.014\n",
      "[37,   200] loss: 0.013\n",
      "[37,   300] loss: 0.010\n",
      "[37,   400] loss: 0.011\n",
      "[37,   500] loss: 0.013\n",
      "[37,   600] loss: 0.013\n",
      "[37,   700] loss: 0.009\n",
      "[37,   800] loss: 0.011\n",
      "[37,   900] loss: 0.014\n",
      "[37,  1000] loss: 0.011\n",
      "[37,  1100] loss: 0.011\n",
      "[37,  1200] loss: 0.013\n",
      "[38,   100] loss: 0.011\n",
      "[38,   200] loss: 0.009\n",
      "[38,   300] loss: 0.008\n",
      "[38,   400] loss: 0.010\n",
      "[38,   500] loss: 0.014\n",
      "[38,   600] loss: 0.013\n",
      "[38,   700] loss: 0.009\n",
      "[38,   800] loss: 0.009\n",
      "[38,   900] loss: 0.011\n",
      "[38,  1000] loss: 0.010\n",
      "[38,  1100] loss: 0.011\n",
      "[38,  1200] loss: 0.013\n",
      "[39,   100] loss: 0.013\n",
      "[39,   200] loss: 0.009\n",
      "[39,   300] loss: 0.018\n",
      "[39,   400] loss: 0.008\n",
      "[39,   500] loss: 0.008\n",
      "[39,   600] loss: 0.011\n",
      "[39,   700] loss: 0.010\n",
      "[39,   800] loss: 0.011\n",
      "[39,   900] loss: 0.011\n",
      "[39,  1000] loss: 0.010\n",
      "[39,  1100] loss: 0.011\n",
      "[39,  1200] loss: 0.013\n",
      "[40,   100] loss: 0.009\n",
      "[40,   200] loss: 0.010\n",
      "[40,   300] loss: 0.011\n",
      "[40,   400] loss: 0.009\n",
      "[40,   500] loss: 0.012\n",
      "[40,   600] loss: 0.012\n",
      "[40,   700] loss: 0.010\n",
      "[40,   800] loss: 0.013\n",
      "[40,   900] loss: 0.008\n",
      "[40,  1000] loss: 0.011\n",
      "[40,  1100] loss: 0.012\n",
      "[40,  1200] loss: 0.014\n",
      "[41,   100] loss: 0.007\n",
      "[41,   200] loss: 0.008\n",
      "[41,   300] loss: 0.007\n",
      "[41,   400] loss: 0.010\n",
      "[41,   500] loss: 0.012\n",
      "[41,   600] loss: 0.009\n",
      "[41,   700] loss: 0.014\n",
      "[41,   800] loss: 0.011\n",
      "[41,   900] loss: 0.010\n",
      "[41,  1000] loss: 0.008\n",
      "[41,  1100] loss: 0.008\n",
      "[41,  1200] loss: 0.010\n",
      "[42,   100] loss: 0.007\n",
      "[42,   200] loss: 0.006\n",
      "[42,   300] loss: 0.011\n",
      "[42,   400] loss: 0.013\n",
      "[42,   500] loss: 0.009\n",
      "[42,   600] loss: 0.010\n",
      "[42,   700] loss: 0.008\n",
      "[42,   800] loss: 0.012\n",
      "[42,   900] loss: 0.011\n",
      "[42,  1000] loss: 0.011\n",
      "[42,  1100] loss: 0.008\n",
      "[42,  1200] loss: 0.008\n",
      "[43,   100] loss: 0.004\n",
      "[43,   200] loss: 0.009\n",
      "[43,   300] loss: 0.009\n",
      "[43,   400] loss: 0.008\n",
      "[43,   500] loss: 0.012\n",
      "[43,   600] loss: 0.007\n",
      "[43,   700] loss: 0.012\n",
      "[43,   800] loss: 0.007\n",
      "[43,   900] loss: 0.011\n",
      "[43,  1000] loss: 0.008\n",
      "[43,  1100] loss: 0.007\n",
      "[43,  1200] loss: 0.011\n",
      "[44,   100] loss: 0.008\n",
      "[44,   200] loss: 0.005\n",
      "[44,   300] loss: 0.007\n",
      "[44,   400] loss: 0.006\n",
      "[44,   500] loss: 0.006\n",
      "[44,   600] loss: 0.011\n",
      "[44,   700] loss: 0.009\n",
      "[44,   800] loss: 0.009\n",
      "[44,   900] loss: 0.007\n",
      "[44,  1000] loss: 0.006\n",
      "[44,  1100] loss: 0.010\n",
      "[44,  1200] loss: 0.018\n",
      "[45,   100] loss: 0.011\n",
      "[45,   200] loss: 0.005\n",
      "[45,   300] loss: 0.008\n",
      "[45,   400] loss: 0.007\n",
      "[45,   500] loss: 0.009\n",
      "[45,   600] loss: 0.007\n",
      "[45,   700] loss: 0.010\n",
      "[45,   800] loss: 0.009\n",
      "[45,   900] loss: 0.006\n",
      "[45,  1000] loss: 0.008\n",
      "[45,  1100] loss: 0.009\n",
      "[45,  1200] loss: 0.010\n",
      "[46,   100] loss: 0.004\n",
      "[46,   200] loss: 0.006\n",
      "[46,   300] loss: 0.009\n",
      "[46,   400] loss: 0.005\n",
      "[46,   500] loss: 0.009\n",
      "[46,   600] loss: 0.007\n",
      "[46,   700] loss: 0.006\n",
      "[46,   800] loss: 0.006\n",
      "[46,   900] loss: 0.007\n",
      "[46,  1000] loss: 0.008\n",
      "[46,  1100] loss: 0.006\n",
      "[46,  1200] loss: 0.010\n",
      "[47,   100] loss: 0.004\n",
      "[47,   200] loss: 0.007\n",
      "[47,   300] loss: 0.007\n",
      "[47,   400] loss: 0.006\n",
      "[47,   500] loss: 0.007\n",
      "[47,   600] loss: 0.007\n",
      "[47,   700] loss: 0.012\n",
      "[47,   800] loss: 0.005\n",
      "[47,   900] loss: 0.008\n",
      "[47,  1000] loss: 0.009\n",
      "[47,  1100] loss: 0.008\n",
      "[47,  1200] loss: 0.008\n",
      "[48,   100] loss: 0.007\n",
      "[48,   200] loss: 0.005\n",
      "[48,   300] loss: 0.006\n",
      "[48,   400] loss: 0.006\n",
      "[48,   500] loss: 0.005\n",
      "[48,   600] loss: 0.006\n",
      "[48,   700] loss: 0.007\n",
      "[48,   800] loss: 0.007\n",
      "[48,   900] loss: 0.007\n",
      "[48,  1000] loss: 0.007\n",
      "[48,  1100] loss: 0.008\n",
      "[48,  1200] loss: 0.007\n",
      "[49,   100] loss: 0.006\n",
      "[49,   200] loss: 0.004\n",
      "[49,   300] loss: 0.004\n",
      "[49,   400] loss: 0.008\n",
      "[49,   500] loss: 0.005\n",
      "[49,   600] loss: 0.007\n",
      "[49,   700] loss: 0.009\n",
      "[49,   800] loss: 0.006\n",
      "[49,   900] loss: 0.007\n",
      "[49,  1000] loss: 0.005\n",
      "[49,  1100] loss: 0.005\n",
      "[49,  1200] loss: 0.008\n",
      "[50,   100] loss: 0.004\n",
      "[50,   200] loss: 0.005\n",
      "[50,   300] loss: 0.004\n",
      "[50,   400] loss: 0.005\n",
      "[50,   500] loss: 0.011\n",
      "[50,   600] loss: 0.007\n",
      "[50,   700] loss: 0.006\n",
      "[50,   800] loss: 0.006\n",
      "[50,   900] loss: 0.004\n",
      "[50,  1000] loss: 0.008\n",
      "[50,  1100] loss: 0.008\n",
      "[50,  1200] loss: 0.005\n",
      "finished training\n"
     ]
    }
   ],
   "source": [
    "#33\n",
    "#change different batch size and number of epochs\n",
    "xtrain, ltrain = MNISTtools.load(dataset = \"training\", path = \"/datasets/MNIST\")\n",
    "xtest, ltest = MNISTtools.load(dataset = \"testing\", path = \"/datasets/MNIST\")\n",
    "xtrain = normalize_MNIST_images(xtrain)\n",
    "xtest = normalize_MNIST_images(xtest)\n",
    "\n",
    "xtrain = xtrain.reshape(28,28,1,60000)\n",
    "xtrain = np.moveaxis(xtrain,-1,0)\n",
    "xtrain = np.moveaxis(xtrain,-1,1)\n",
    "xtest = xtest.reshape(28,28,1,10000)\n",
    "xtest = np.moveaxis(xtest,-1,0)\n",
    "xtest = np.moveaxis(xtest,-1,1)\n",
    "#xtrain and xtest add cuda to run in GPU\n",
    "xtrain = ag.Variable(torch.from_numpy(xtrain).cuda(),requires_grad = True)\n",
    "ltrain = ag.Variable(torch.from_numpy(ltrain),requires_grad = False)\n",
    "xtest = ag.Variable(torch.from_numpy(xtest).cuda(), requires_grad = True)\n",
    "#net add cuda to run in GPU\n",
    "net = LeNet()\n",
    "net.cuda()\n",
    "#decrease B from 100 to 50 and increase epoch(T) from 10 to 50\n",
    "N = xtrain.size()[0]\n",
    "B = 50\n",
    "NB = N/B\n",
    "T = 50\n",
    "gamma = .001\n",
    "rho = .9\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr = gamma, momentum = rho)\n",
    "\n",
    "for epoch in range(T):\n",
    "    running_loss = 0.0\n",
    "    idxminibatches = np.random.permutation(NB)\n",
    "    for k in range(NB):\n",
    "        i = idxminibatches[k]\n",
    "        idxsmp = range(B*i, B*(i+1))\n",
    "        inputs = xtrain[idxsmp].cuda()\n",
    "        labels = ltrain[idxsmp].cuda()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        running_loss += loss[0]\n",
    "        if k % 100 == 99:\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, k + 1, running_loss/100))\n",
    "            running_loss = 0.0\n",
    "            \n",
    "print('finished training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.96000000000001\n",
      "by adding more epoch, we can get lower loss and slightly increase the detection rate.\n"
     ]
    }
   ],
   "source": [
    "yinit = net.forward(xtest)\n",
    "print(100 * np.mean(ltest == yinit.data.cpu().numpy().T.argmax(axis=0)))\n",
    "print(\"by adding more epoch, we can get lower loss and slightly increase the detection rate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 2.298\n",
      "[1,   200] loss: 2.266\n",
      "[1,   300] loss: 1.770\n",
      "[1,   400] loss: 0.780\n",
      "[1,   500] loss: 0.480\n",
      "[1,   600] loss: 0.342\n",
      "[1,   700] loss: 0.292\n",
      "[1,   800] loss: 0.269\n",
      "[1,   900] loss: 0.288\n",
      "[1,  1000] loss: 0.230\n",
      "[1,  1100] loss: 0.206\n",
      "[1,  1200] loss: 0.167\n",
      "[1,  1300] loss: 0.153\n",
      "[1,  1400] loss: 0.164\n",
      "[1,  1500] loss: 0.137\n",
      "[1,  1600] loss: 0.152\n",
      "[1,  1700] loss: 0.187\n",
      "[1,  1800] loss: 0.169\n",
      "[1,  1900] loss: 0.136\n",
      "[1,  2000] loss: 0.141\n",
      "[1,  2100] loss: 0.140\n",
      "[1,  2200] loss: 0.108\n",
      "[1,  2300] loss: 0.133\n",
      "[1,  2400] loss: 0.117\n",
      "[1,  2500] loss: 0.096\n",
      "[1,  2600] loss: 0.154\n",
      "[1,  2700] loss: 0.107\n",
      "[1,  2800] loss: 0.113\n",
      "[1,  2900] loss: 0.095\n",
      "[1,  3000] loss: 0.113\n",
      "[1,  3100] loss: 0.104\n",
      "[1,  3200] loss: 0.126\n",
      "[1,  3300] loss: 0.090\n",
      "[1,  3400] loss: 0.059\n",
      "[1,  3500] loss: 0.109\n",
      "[1,  3600] loss: 0.088\n",
      "[1,  3700] loss: 0.103\n",
      "[1,  3800] loss: 0.099\n",
      "[1,  3900] loss: 0.091\n",
      "[1,  4000] loss: 0.093\n",
      "[1,  4100] loss: 0.088\n",
      "[1,  4200] loss: 0.121\n",
      "[1,  4300] loss: 0.094\n",
      "[1,  4400] loss: 0.077\n",
      "[1,  4500] loss: 0.064\n",
      "[1,  4600] loss: 0.074\n",
      "[1,  4700] loss: 0.073\n",
      "[1,  4800] loss: 0.092\n",
      "[1,  4900] loss: 0.054\n",
      "[1,  5000] loss: 0.069\n",
      "[1,  5100] loss: 0.074\n",
      "[1,  5200] loss: 0.075\n",
      "[1,  5300] loss: 0.074\n",
      "[1,  5400] loss: 0.095\n",
      "[1,  5500] loss: 0.108\n",
      "[1,  5600] loss: 0.074\n",
      "[1,  5700] loss: 0.104\n",
      "[1,  5800] loss: 0.096\n",
      "[1,  5900] loss: 0.092\n",
      "[1,  6000] loss: 0.060\n",
      "[2,   100] loss: 0.040\n",
      "[2,   200] loss: 0.070\n",
      "[2,   300] loss: 0.078\n",
      "[2,   400] loss: 0.059\n",
      "[2,   500] loss: 0.061\n",
      "[2,   600] loss: 0.041\n",
      "[2,   700] loss: 0.070\n",
      "[2,   800] loss: 0.058\n",
      "[2,   900] loss: 0.047\n",
      "[2,  1000] loss: 0.073\n",
      "[2,  1100] loss: 0.050\n",
      "[2,  1200] loss: 0.046\n",
      "[2,  1300] loss: 0.073\n",
      "[2,  1400] loss: 0.058\n",
      "[2,  1500] loss: 0.088\n",
      "[2,  1600] loss: 0.072\n",
      "[2,  1700] loss: 0.054\n",
      "[2,  1800] loss: 0.054\n",
      "[2,  1900] loss: 0.076\n",
      "[2,  2000] loss: 0.075\n",
      "[2,  2100] loss: 0.039\n",
      "[2,  2200] loss: 0.057\n",
      "[2,  2300] loss: 0.059\n",
      "[2,  2400] loss: 0.056\n",
      "[2,  2500] loss: 0.063\n",
      "[2,  2600] loss: 0.076\n",
      "[2,  2700] loss: 0.054\n",
      "[2,  2800] loss: 0.054\n",
      "[2,  2900] loss: 0.063\n",
      "[2,  3000] loss: 0.066\n",
      "[2,  3100] loss: 0.057\n",
      "[2,  3200] loss: 0.058\n",
      "[2,  3300] loss: 0.049\n",
      "[2,  3400] loss: 0.030\n",
      "[2,  3500] loss: 0.037\n",
      "[2,  3600] loss: 0.048\n",
      "[2,  3700] loss: 0.055\n",
      "[2,  3800] loss: 0.051\n",
      "[2,  3900] loss: 0.057\n",
      "[2,  4000] loss: 0.066\n",
      "[2,  4100] loss: 0.046\n",
      "[2,  4200] loss: 0.050\n",
      "[2,  4300] loss: 0.054\n",
      "[2,  4400] loss: 0.052\n",
      "[2,  4500] loss: 0.056\n",
      "[2,  4600] loss: 0.064\n",
      "[2,  4700] loss: 0.052\n",
      "[2,  4800] loss: 0.071\n",
      "[2,  4900] loss: 0.059\n",
      "[2,  5000] loss: 0.061\n",
      "[2,  5100] loss: 0.063\n",
      "[2,  5200] loss: 0.062\n",
      "[2,  5300] loss: 0.054\n",
      "[2,  5400] loss: 0.045\n",
      "[2,  5500] loss: 0.043\n",
      "[2,  5600] loss: 0.048\n",
      "[2,  5700] loss: 0.045\n",
      "[2,  5800] loss: 0.041\n",
      "[2,  5900] loss: 0.064\n",
      "[2,  6000] loss: 0.034\n",
      "[3,   100] loss: 0.027\n",
      "[3,   200] loss: 0.042\n",
      "[3,   300] loss: 0.044\n",
      "[3,   400] loss: 0.028\n",
      "[3,   500] loss: 0.028\n",
      "[3,   600] loss: 0.055\n",
      "[3,   700] loss: 0.053\n",
      "[3,   800] loss: 0.040\n",
      "[3,   900] loss: 0.056\n",
      "[3,  1000] loss: 0.049\n",
      "[3,  1100] loss: 0.022\n",
      "[3,  1200] loss: 0.019\n",
      "[3,  1300] loss: 0.047\n",
      "[3,  1400] loss: 0.022\n",
      "[3,  1500] loss: 0.048\n",
      "[3,  1600] loss: 0.030\n",
      "[3,  1700] loss: 0.039\n",
      "[3,  1800] loss: 0.032\n",
      "[3,  1900] loss: 0.020\n",
      "[3,  2000] loss: 0.031\n",
      "[3,  2100] loss: 0.059\n",
      "[3,  2200] loss: 0.032\n",
      "[3,  2300] loss: 0.055\n",
      "[3,  2400] loss: 0.039\n",
      "[3,  2500] loss: 0.063\n",
      "[3,  2600] loss: 0.054\n",
      "[3,  2700] loss: 0.022\n",
      "[3,  2800] loss: 0.026\n",
      "[3,  2900] loss: 0.048\n",
      "[3,  3000] loss: 0.029\n",
      "[3,  3100] loss: 0.042\n",
      "[3,  3200] loss: 0.031\n",
      "[3,  3300] loss: 0.045\n",
      "[3,  3400] loss: 0.052\n",
      "[3,  3500] loss: 0.038\n",
      "[3,  3600] loss: 0.029\n",
      "[3,  3700] loss: 0.064\n",
      "[3,  3800] loss: 0.040\n",
      "[3,  3900] loss: 0.037\n",
      "[3,  4000] loss: 0.042\n",
      "[3,  4100] loss: 0.042\n",
      "[3,  4200] loss: 0.023\n",
      "[3,  4300] loss: 0.069\n",
      "[3,  4400] loss: 0.055\n",
      "[3,  4500] loss: 0.031\n",
      "[3,  4600] loss: 0.029\n",
      "[3,  4700] loss: 0.048\n",
      "[3,  4800] loss: 0.053\n",
      "[3,  4900] loss: 0.048\n",
      "[3,  5000] loss: 0.049\n",
      "[3,  5100] loss: 0.029\n",
      "[3,  5200] loss: 0.038\n",
      "[3,  5300] loss: 0.051\n",
      "[3,  5400] loss: 0.067\n",
      "[3,  5500] loss: 0.037\n",
      "[3,  5600] loss: 0.045\n",
      "[3,  5700] loss: 0.048\n",
      "[3,  5800] loss: 0.036\n",
      "[3,  5900] loss: 0.047\n",
      "[3,  6000] loss: 0.038\n",
      "[4,   100] loss: 0.026\n",
      "[4,   200] loss: 0.022\n",
      "[4,   300] loss: 0.026\n",
      "[4,   400] loss: 0.017\n",
      "[4,   500] loss: 0.018\n",
      "[4,   600] loss: 0.049\n",
      "[4,   700] loss: 0.028\n",
      "[4,   800] loss: 0.015\n",
      "[4,   900] loss: 0.014\n",
      "[4,  1000] loss: 0.032\n",
      "[4,  1100] loss: 0.034\n",
      "[4,  1200] loss: 0.034\n",
      "[4,  1300] loss: 0.032\n",
      "[4,  1400] loss: 0.028\n",
      "[4,  1500] loss: 0.035\n",
      "[4,  1600] loss: 0.050\n",
      "[4,  1700] loss: 0.021\n",
      "[4,  1800] loss: 0.022\n",
      "[4,  1900] loss: 0.028\n",
      "[4,  2000] loss: 0.042\n",
      "[4,  2100] loss: 0.024\n",
      "[4,  2200] loss: 0.026\n",
      "[4,  2300] loss: 0.030\n",
      "[4,  2400] loss: 0.036\n",
      "[4,  2500] loss: 0.034\n",
      "[4,  2600] loss: 0.024\n",
      "[4,  2700] loss: 0.033\n",
      "[4,  2800] loss: 0.021\n",
      "[4,  2900] loss: 0.039\n",
      "[4,  3000] loss: 0.038\n",
      "[4,  3100] loss: 0.033\n",
      "[4,  3200] loss: 0.041\n",
      "[4,  3300] loss: 0.033\n",
      "[4,  3400] loss: 0.020\n",
      "[4,  3500] loss: 0.047\n",
      "[4,  3600] loss: 0.039\n",
      "[4,  3700] loss: 0.027\n",
      "[4,  3800] loss: 0.033\n",
      "[4,  3900] loss: 0.051\n",
      "[4,  4000] loss: 0.016\n",
      "[4,  4100] loss: 0.030\n",
      "[4,  4200] loss: 0.040\n",
      "[4,  4300] loss: 0.048\n",
      "[4,  4400] loss: 0.032\n",
      "[4,  4500] loss: 0.053\n",
      "[4,  4600] loss: 0.029\n",
      "[4,  4700] loss: 0.039\n",
      "[4,  4800] loss: 0.037\n",
      "[4,  4900] loss: 0.027\n",
      "[4,  5000] loss: 0.038\n",
      "[4,  5100] loss: 0.025\n",
      "[4,  5200] loss: 0.052\n",
      "[4,  5300] loss: 0.024\n",
      "[4,  5400] loss: 0.024\n",
      "[4,  5500] loss: 0.022\n",
      "[4,  5600] loss: 0.027\n",
      "[4,  5700] loss: 0.037\n",
      "[4,  5800] loss: 0.030\n",
      "[4,  5900] loss: 0.030\n",
      "[4,  6000] loss: 0.027\n",
      "[5,   100] loss: 0.018\n",
      "[5,   200] loss: 0.016\n",
      "[5,   300] loss: 0.021\n",
      "[5,   400] loss: 0.021\n",
      "[5,   500] loss: 0.039\n",
      "[5,   600] loss: 0.017\n",
      "[5,   700] loss: 0.032\n",
      "[5,   800] loss: 0.036\n",
      "[5,   900] loss: 0.029\n",
      "[5,  1000] loss: 0.031\n",
      "[5,  1100] loss: 0.021\n",
      "[5,  1200] loss: 0.023\n",
      "[5,  1300] loss: 0.040\n",
      "[5,  1400] loss: 0.025\n",
      "[5,  1500] loss: 0.012\n",
      "[5,  1600] loss: 0.022\n",
      "[5,  1700] loss: 0.021\n",
      "[5,  1800] loss: 0.026\n",
      "[5,  1900] loss: 0.019\n",
      "[5,  2000] loss: 0.028\n",
      "[5,  2100] loss: 0.031\n",
      "[5,  2200] loss: 0.062\n",
      "[5,  2300] loss: 0.011\n",
      "[5,  2400] loss: 0.025\n",
      "[5,  2500] loss: 0.015\n",
      "[5,  2600] loss: 0.024\n",
      "[5,  2700] loss: 0.021\n",
      "[5,  2800] loss: 0.023\n",
      "[5,  2900] loss: 0.023\n",
      "[5,  3000] loss: 0.027\n",
      "[5,  3100] loss: 0.026\n",
      "[5,  3200] loss: 0.045\n",
      "[5,  3300] loss: 0.013\n",
      "[5,  3400] loss: 0.008\n",
      "[5,  3500] loss: 0.036\n",
      "[5,  3600] loss: 0.036\n",
      "[5,  3700] loss: 0.018\n",
      "[5,  3800] loss: 0.025\n",
      "[5,  3900] loss: 0.020\n",
      "[5,  4000] loss: 0.021\n",
      "[5,  4100] loss: 0.018\n",
      "[5,  4200] loss: 0.010\n",
      "[5,  4300] loss: 0.021\n",
      "[5,  4400] loss: 0.032\n",
      "[5,  4500] loss: 0.014\n",
      "[5,  4600] loss: 0.041\n",
      "[5,  4700] loss: 0.020\n",
      "[5,  4800] loss: 0.028\n",
      "[5,  4900] loss: 0.021\n",
      "[5,  5000] loss: 0.020\n",
      "[5,  5100] loss: 0.040\n",
      "[5,  5200] loss: 0.027\n",
      "[5,  5300] loss: 0.035\n",
      "[5,  5400] loss: 0.032\n",
      "[5,  5500] loss: 0.052\n",
      "[5,  5600] loss: 0.038\n",
      "[5,  5700] loss: 0.025\n",
      "[5,  5800] loss: 0.035\n",
      "[5,  5900] loss: 0.022\n",
      "[5,  6000] loss: 0.025\n",
      "[6,   100] loss: 0.020\n",
      "[6,   200] loss: 0.021\n",
      "[6,   300] loss: 0.017\n",
      "[6,   400] loss: 0.024\n",
      "[6,   500] loss: 0.029\n",
      "[6,   600] loss: 0.030\n",
      "[6,   700] loss: 0.013\n",
      "[6,   800] loss: 0.020\n",
      "[6,   900] loss: 0.017\n",
      "[6,  1000] loss: 0.022\n",
      "[6,  1100] loss: 0.013\n",
      "[6,  1200] loss: 0.014\n",
      "[6,  1300] loss: 0.024\n",
      "[6,  1400] loss: 0.014\n",
      "[6,  1500] loss: 0.024\n",
      "[6,  1600] loss: 0.015\n",
      "[6,  1700] loss: 0.039\n",
      "[6,  1800] loss: 0.016\n",
      "[6,  1900] loss: 0.016\n",
      "[6,  2000] loss: 0.029\n",
      "[6,  2100] loss: 0.015\n",
      "[6,  2200] loss: 0.018\n",
      "[6,  2300] loss: 0.035\n",
      "[6,  2400] loss: 0.013\n",
      "[6,  2500] loss: 0.017\n",
      "[6,  2600] loss: 0.024\n",
      "[6,  2700] loss: 0.020\n",
      "[6,  2800] loss: 0.028\n",
      "[6,  2900] loss: 0.013\n",
      "[6,  3000] loss: 0.016\n",
      "[6,  3100] loss: 0.010\n",
      "[6,  3200] loss: 0.046\n",
      "[6,  3300] loss: 0.023\n",
      "[6,  3400] loss: 0.016\n",
      "[6,  3500] loss: 0.031\n",
      "[6,  3600] loss: 0.029\n",
      "[6,  3700] loss: 0.016\n",
      "[6,  3800] loss: 0.023\n",
      "[6,  3900] loss: 0.018\n",
      "[6,  4000] loss: 0.029\n",
      "[6,  4100] loss: 0.019\n",
      "[6,  4200] loss: 0.023\n",
      "[6,  4300] loss: 0.021\n",
      "[6,  4400] loss: 0.020\n",
      "[6,  4500] loss: 0.028\n",
      "[6,  4600] loss: 0.030\n",
      "[6,  4700] loss: 0.021\n",
      "[6,  4800] loss: 0.026\n",
      "[6,  4900] loss: 0.025\n",
      "[6,  5000] loss: 0.023\n",
      "[6,  5100] loss: 0.025\n",
      "[6,  5200] loss: 0.021\n",
      "[6,  5300] loss: 0.025\n",
      "[6,  5400] loss: 0.029\n",
      "[6,  5500] loss: 0.022\n",
      "[6,  5600] loss: 0.034\n",
      "[6,  5700] loss: 0.022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6,  5800] loss: 0.035\n",
      "[6,  5900] loss: 0.026\n",
      "[6,  6000] loss: 0.034\n",
      "[7,   100] loss: 0.010\n",
      "[7,   200] loss: 0.011\n",
      "[7,   300] loss: 0.015\n",
      "[7,   400] loss: 0.014\n",
      "[7,   500] loss: 0.028\n",
      "[7,   600] loss: 0.012\n",
      "[7,   700] loss: 0.020\n",
      "[7,   800] loss: 0.014\n",
      "[7,   900] loss: 0.012\n",
      "[7,  1000] loss: 0.015\n",
      "[7,  1100] loss: 0.018\n",
      "[7,  1200] loss: 0.014\n",
      "[7,  1300] loss: 0.014\n",
      "[7,  1400] loss: 0.011\n",
      "[7,  1500] loss: 0.029\n",
      "[7,  1600] loss: 0.009\n",
      "[7,  1700] loss: 0.011\n",
      "[7,  1800] loss: 0.013\n",
      "[7,  1900] loss: 0.014\n",
      "[7,  2000] loss: 0.012\n",
      "[7,  2100] loss: 0.009\n",
      "[7,  2200] loss: 0.030\n",
      "[7,  2300] loss: 0.032\n",
      "[7,  2400] loss: 0.015\n",
      "[7,  2500] loss: 0.020\n",
      "[7,  2600] loss: 0.013\n",
      "[7,  2700] loss: 0.031\n",
      "[7,  2800] loss: 0.028\n",
      "[7,  2900] loss: 0.036\n",
      "[7,  3000] loss: 0.027\n",
      "[7,  3100] loss: 0.006\n",
      "[7,  3200] loss: 0.026\n",
      "[7,  3300] loss: 0.023\n",
      "[7,  3400] loss: 0.010\n",
      "[7,  3500] loss: 0.028\n",
      "[7,  3600] loss: 0.011\n",
      "[7,  3700] loss: 0.029\n",
      "[7,  3800] loss: 0.025\n",
      "[7,  3900] loss: 0.026\n",
      "[7,  4000] loss: 0.019\n",
      "[7,  4100] loss: 0.007\n",
      "[7,  4200] loss: 0.013\n",
      "[7,  4300] loss: 0.013\n",
      "[7,  4400] loss: 0.046\n",
      "[7,  4500] loss: 0.022\n",
      "[7,  4600] loss: 0.022\n",
      "[7,  4700] loss: 0.007\n",
      "[7,  4800] loss: 0.014\n",
      "[7,  4900] loss: 0.033\n",
      "[7,  5000] loss: 0.032\n",
      "[7,  5100] loss: 0.014\n",
      "[7,  5200] loss: 0.020\n",
      "[7,  5300] loss: 0.014\n",
      "[7,  5400] loss: 0.014\n",
      "[7,  5500] loss: 0.013\n",
      "[7,  5600] loss: 0.022\n",
      "[7,  5700] loss: 0.024\n",
      "[7,  5800] loss: 0.014\n",
      "[7,  5900] loss: 0.024\n",
      "[7,  6000] loss: 0.013\n",
      "[8,   100] loss: 0.006\n",
      "[8,   200] loss: 0.016\n",
      "[8,   300] loss: 0.025\n",
      "[8,   400] loss: 0.008\n",
      "[8,   500] loss: 0.017\n",
      "[8,   600] loss: 0.009\n",
      "[8,   700] loss: 0.014\n",
      "[8,   800] loss: 0.007\n",
      "[8,   900] loss: 0.014\n",
      "[8,  1000] loss: 0.027\n",
      "[8,  1100] loss: 0.009\n",
      "[8,  1200] loss: 0.022\n",
      "[8,  1300] loss: 0.014\n",
      "[8,  1400] loss: 0.010\n",
      "[8,  1500] loss: 0.013\n",
      "[8,  1600] loss: 0.016\n",
      "[8,  1700] loss: 0.010\n",
      "[8,  1800] loss: 0.017\n",
      "[8,  1900] loss: 0.018\n",
      "[8,  2000] loss: 0.024\n",
      "[8,  2100] loss: 0.015\n",
      "[8,  2200] loss: 0.025\n",
      "[8,  2300] loss: 0.020\n",
      "[8,  2400] loss: 0.020\n",
      "[8,  2500] loss: 0.034\n",
      "[8,  2600] loss: 0.018\n",
      "[8,  2700] loss: 0.014\n",
      "[8,  2800] loss: 0.007\n",
      "[8,  2900] loss: 0.013\n",
      "[8,  3000] loss: 0.019\n",
      "[8,  3100] loss: 0.011\n",
      "[8,  3200] loss: 0.005\n",
      "[8,  3300] loss: 0.022\n",
      "[8,  3400] loss: 0.008\n",
      "[8,  3500] loss: 0.015\n",
      "[8,  3600] loss: 0.033\n",
      "[8,  3700] loss: 0.022\n",
      "[8,  3800] loss: 0.018\n",
      "[8,  3900] loss: 0.008\n",
      "[8,  4000] loss: 0.009\n",
      "[8,  4100] loss: 0.035\n",
      "[8,  4200] loss: 0.032\n",
      "[8,  4300] loss: 0.019\n",
      "[8,  4400] loss: 0.024\n",
      "[8,  4500] loss: 0.017\n",
      "[8,  4600] loss: 0.012\n",
      "[8,  4700] loss: 0.006\n",
      "[8,  4800] loss: 0.019\n",
      "[8,  4900] loss: 0.006\n",
      "[8,  5000] loss: 0.013\n",
      "[8,  5100] loss: 0.020\n",
      "[8,  5200] loss: 0.012\n",
      "[8,  5300] loss: 0.007\n",
      "[8,  5400] loss: 0.012\n",
      "[8,  5500] loss: 0.028\n",
      "[8,  5600] loss: 0.026\n",
      "[8,  5700] loss: 0.027\n",
      "[8,  5800] loss: 0.015\n",
      "[8,  5900] loss: 0.013\n",
      "[8,  6000] loss: 0.022\n",
      "[9,   100] loss: 0.006\n",
      "[9,   200] loss: 0.014\n",
      "[9,   300] loss: 0.024\n",
      "[9,   400] loss: 0.010\n",
      "[9,   500] loss: 0.006\n",
      "[9,   600] loss: 0.006\n",
      "[9,   700] loss: 0.005\n",
      "[9,   800] loss: 0.008\n",
      "[9,   900] loss: 0.013\n",
      "[9,  1000] loss: 0.014\n",
      "[9,  1100] loss: 0.012\n",
      "[9,  1200] loss: 0.005\n",
      "[9,  1300] loss: 0.008\n",
      "[9,  1400] loss: 0.003\n",
      "[9,  1500] loss: 0.013\n",
      "[9,  1600] loss: 0.005\n",
      "[9,  1700] loss: 0.018\n",
      "[9,  1800] loss: 0.009\n",
      "[9,  1900] loss: 0.008\n",
      "[9,  2000] loss: 0.004\n",
      "[9,  2100] loss: 0.019\n",
      "[9,  2200] loss: 0.009\n",
      "[9,  2300] loss: 0.014\n",
      "[9,  2400] loss: 0.011\n",
      "[9,  2500] loss: 0.006\n",
      "[9,  2600] loss: 0.023\n",
      "[9,  2700] loss: 0.019\n",
      "[9,  2800] loss: 0.008\n",
      "[9,  2900] loss: 0.012\n",
      "[9,  3000] loss: 0.009\n",
      "[9,  3100] loss: 0.016\n",
      "[9,  3200] loss: 0.021\n",
      "[9,  3300] loss: 0.017\n",
      "[9,  3400] loss: 0.019\n",
      "[9,  3500] loss: 0.016\n",
      "[9,  3600] loss: 0.020\n",
      "[9,  3700] loss: 0.011\n",
      "[9,  3800] loss: 0.009\n",
      "[9,  3900] loss: 0.029\n",
      "[9,  4000] loss: 0.028\n",
      "[9,  4100] loss: 0.029\n",
      "[9,  4200] loss: 0.025\n",
      "[9,  4300] loss: 0.014\n",
      "[9,  4400] loss: 0.016\n",
      "[9,  4500] loss: 0.016\n",
      "[9,  4600] loss: 0.014\n",
      "[9,  4700] loss: 0.013\n",
      "[9,  4800] loss: 0.008\n",
      "[9,  4900] loss: 0.011\n",
      "[9,  5000] loss: 0.011\n",
      "[9,  5100] loss: 0.018\n",
      "[9,  5200] loss: 0.022\n",
      "[9,  5300] loss: 0.013\n",
      "[9,  5400] loss: 0.010\n",
      "[9,  5500] loss: 0.008\n",
      "[9,  5600] loss: 0.013\n",
      "[9,  5700] loss: 0.007\n",
      "[9,  5800] loss: 0.018\n",
      "[9,  5900] loss: 0.014\n",
      "[9,  6000] loss: 0.004\n",
      "[10,   100] loss: 0.015\n",
      "[10,   200] loss: 0.018\n",
      "[10,   300] loss: 0.017\n",
      "[10,   400] loss: 0.014\n",
      "[10,   500] loss: 0.009\n",
      "[10,   600] loss: 0.017\n",
      "[10,   700] loss: 0.006\n",
      "[10,   800] loss: 0.013\n",
      "[10,   900] loss: 0.021\n",
      "[10,  1000] loss: 0.012\n",
      "[10,  1100] loss: 0.017\n",
      "[10,  1200] loss: 0.011\n",
      "[10,  1300] loss: 0.008\n",
      "[10,  1400] loss: 0.005\n",
      "[10,  1500] loss: 0.028\n",
      "[10,  1600] loss: 0.009\n",
      "[10,  1700] loss: 0.013\n",
      "[10,  1800] loss: 0.007\n",
      "[10,  1900] loss: 0.005\n",
      "[10,  2000] loss: 0.003\n",
      "[10,  2100] loss: 0.008\n",
      "[10,  2200] loss: 0.016\n",
      "[10,  2300] loss: 0.007\n",
      "[10,  2400] loss: 0.008\n",
      "[10,  2500] loss: 0.002\n",
      "[10,  2600] loss: 0.001\n",
      "[10,  2700] loss: 0.008\n",
      "[10,  2800] loss: 0.007\n",
      "[10,  2900] loss: 0.013\n",
      "[10,  3000] loss: 0.015\n",
      "[10,  3100] loss: 0.012\n",
      "[10,  3200] loss: 0.007\n",
      "[10,  3300] loss: 0.006\n",
      "[10,  3400] loss: 0.010\n",
      "[10,  3500] loss: 0.024\n",
      "[10,  3600] loss: 0.010\n",
      "[10,  3700] loss: 0.024\n",
      "[10,  3800] loss: 0.012\n",
      "[10,  3900] loss: 0.003\n",
      "[10,  4000] loss: 0.017\n",
      "[10,  4100] loss: 0.010\n",
      "[10,  4200] loss: 0.012\n",
      "[10,  4300] loss: 0.006\n",
      "[10,  4400] loss: 0.007\n",
      "[10,  4500] loss: 0.015\n",
      "[10,  4600] loss: 0.022\n",
      "[10,  4700] loss: 0.016\n",
      "[10,  4800] loss: 0.031\n",
      "[10,  4900] loss: 0.040\n",
      "[10,  5000] loss: 0.017\n",
      "[10,  5100] loss: 0.008\n",
      "[10,  5200] loss: 0.009\n",
      "[10,  5300] loss: 0.006\n",
      "[10,  5400] loss: 0.010\n",
      "[10,  5500] loss: 0.026\n",
      "[10,  5600] loss: 0.016\n",
      "[10,  5700] loss: 0.014\n",
      "[10,  5800] loss: 0.011\n",
      "[10,  5900] loss: 0.037\n",
      "[10,  6000] loss: 0.014\n",
      "finished training\n"
     ]
    }
   ],
   "source": [
    "#33\n",
    "#change different learning rate and decrease the batch\n",
    "xtrain, ltrain = MNISTtools.load(dataset = \"training\", path = \"/datasets/MNIST\")\n",
    "xtest, ltest = MNISTtools.load(dataset = \"testing\", path = \"/datasets/MNIST\")\n",
    "xtrain = normalize_MNIST_images(xtrain)\n",
    "xtest = normalize_MNIST_images(xtest)\n",
    "\n",
    "xtrain = xtrain.reshape(28,28,1,60000)\n",
    "xtrain = np.moveaxis(xtrain,-1,0)\n",
    "xtrain = np.moveaxis(xtrain,-1,1)\n",
    "xtest = xtest.reshape(28,28,1,10000)\n",
    "xtest = np.moveaxis(xtest,-1,0)\n",
    "xtest = np.moveaxis(xtest,-1,1)\n",
    "#xtrain and xtest add cuda to run in GPU\n",
    "xtrain = ag.Variable(torch.from_numpy(xtrain).cuda(),requires_grad = True)\n",
    "ltrain = ag.Variable(torch.from_numpy(ltrain),requires_grad = False)\n",
    "xtest = ag.Variable(torch.from_numpy(xtest).cuda(), requires_grad = True)\n",
    "#net add cuda to run in GPU\n",
    "net = LeNet()\n",
    "net.cuda()\n",
    "#decrease B from 100 to 10 and increase learning rate from 0.001 to 0.004\n",
    "N = xtrain.size()[0]\n",
    "B = 10\n",
    "NB = N/B\n",
    "T = 10\n",
    "gamma = .004\n",
    "rho = .9\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr = gamma, momentum = rho)\n",
    "count = 0\n",
    "x_plot = []\n",
    "y_plot = []\n",
    "for epoch in range(T):\n",
    "    running_loss = 0.0\n",
    "    idxminibatches = np.random.permutation(NB)\n",
    "    for k in range(NB):\n",
    "        i = idxminibatches[k]\n",
    "        idxsmp = range(B*i, B*(i+1))\n",
    "        inputs = xtrain[idxsmp].cuda()\n",
    "        labels = ltrain[idxsmp].cuda()\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        running_loss += loss[0]\n",
    "        if k % 100 == 99:\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, k + 1, running_loss/100))\n",
    "            y_plot.append(running_loss.data.cpu().numpy()/100)\n",
    "            x_plot.append(count)\n",
    "            count = count + 1\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('finished training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_plot,y_plot)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.94\n",
      "by increase the learning rate and lower the number of batch, we can get better detection rate as well.\n",
      "Since the learning rate increase, we can see that the loss will fall faster at the beginnering.\n",
      "Also we will see some cuvre when we increase the learning rate\n"
     ]
    }
   ],
   "source": [
    "yinit = net.forward(xtest)\n",
    "print(100 * np.mean(ltest == yinit.data.cpu().numpy().T.argmax(axis=0)))\n",
    "print(\"by increase the learning rate and lower the number of batch, we can get better detection rate as well.\")\n",
    "print(\"Since the learning rate increase, we can see that the loss will fall faster at the beginnering.\")\n",
    "print(\"Also we will see some cuvre when we increase the learning rate\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
